---
title: Datasets
resources:
  - ../src/fysisk_biokemi/datasets/files/**
  - ../src/fysisk_biokemi/datasets/metadata.yml
---

The datasets used in the course exercises are organized by week. Click on the filenames to download the data files.

```{python}
#| echo: false
import yaml
import re
import os
from importlib.resources import files
from pathlib import Path

# Load metadata
dataset_directory = files('fysisk_biokemi.datasets.files')
dataset_files_path = str(dataset_directory)
metadata_file_path = Path(dataset_directory).parent / 'metadata.yml'

with open(metadata_file_path, 'r') as f:
    metadata = yaml.safe_load(f)
```

```{python}
#| echo: false
def get_exercise_title(exercise_filename):
    """Extract exercise title from the exercise file."""
    if not exercise_filename:
        return "—"
    
    exercise_path = f"lessons/exercises/{exercise_filename}"
    if not os.path.exists(exercise_path):
        return "—"
    
    try:
        with open(exercise_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # Look for the first ## header (exercise title)
        match = re.search(r'^##\s+(.+)', content, re.MULTILINE)
        if match:
            return match.group(1).strip()
        else:
            # Fallback to filename conversion
            return exercise_filename.replace('.qmd', '').replace('-', ' ').title()
    except:
        # Fallback to filename conversion
        return exercise_filename.replace('.qmd', '').replace('-', ' ').title()

def print_dataset_row(dataset):
    """Print a table row for a dataset."""
    github_raw_url = f"https://raw.githubusercontent.com/au-mbg/fysisk-biokemi/main/src/fysisk_biokemi/datasets/files/{dataset['filename']}"
    download_link = f"[{dataset['filename']}]({github_raw_url})"
    exercise_title = get_exercise_title(dataset['exercise'])
    description = dataset['description']
    
    print(f"| {download_link} | {exercise_title} | {description} |")
```

```{python}
#| echo: false
# Group datasets by week
datasets_by_week = {}
for filename, info in metadata['datasets'].items():
    week = info['week']
    if week is not None:  # Only include datasets assigned to weeks
        if week not in datasets_by_week:
            datasets_by_week[week] = []
        datasets_by_week[week].append({
            'filename': filename,
            'exercise': info['exercise'],
            'description': info['description'],
            'original_name': info.get('original_name', filename)
        })

# Collect unassigned datasets
unassigned = []
for filename, info in metadata['datasets'].items():
    if info['week'] is None:
        unassigned.append({
            'filename': filename,
            'exercise': info['exercise'],
            'description': info['description']
        })
```

```{python}
#| echo: false
#| output: asis
# Generate markdown for each week
sorted_weeks = sorted(datasets_by_week.keys())

for week in sorted_weeks:
    print(f"## Week {week}\n")
    
    datasets = datasets_by_week[week]
    
    # Create table header
    print("| Dataset | Exercise | Description |")
    print("|---------|----------|-------------|")
    
    # Add each dataset as a table row
    for dataset in datasets:
        print_dataset_row(dataset)
    
    print("")  # Empty line after each week section
```

```{python}
#| echo: false
#| output: asis
# Show unassigned datasets
if unassigned:
    print("## Additional Datasets\n")
    print("*These datasets are available but not yet assigned to specific weeks.*\n")
    
    print("| Dataset | Exercise | Description |")
    print("|---------|----------|-------------|")
    
    for dataset in unassigned:
        print_dataset_row(dataset)
```